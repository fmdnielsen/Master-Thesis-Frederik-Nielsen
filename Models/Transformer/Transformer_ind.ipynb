{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e228a33-6784-458c-be3f-121e341d90aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0e81e-bd7c-4c22-a600-d62ba48ccfa9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "name = \"ind_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b482f9-df4e-46e5-8882-cd1aa30704de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "data_type = \"ind_final\"\n",
    "# seed = 42\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01c33a-c36c-454e-aa52-e214b702d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/work/Master project/data/data_5_clean_lstm.csv\")\n",
    "\n",
    "df = df\n",
    "df = df.set_index('date')\n",
    "\n",
    "num_cols = ['air_temp','humidity','solar_radiation','dew_point_temp']\n",
    "remainder_cols = ['hour_sin','hour_cos','week_sin','week_cos','month_sin','month_cos','day_of_the_week_sin','day_of_the_week_cos','is_weekend' ]\n",
    "\n",
    "print(\"Min Date:\", df.index.min())\n",
    "print(\"Max Date:\", df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba9842-d8c9-499b-9605-c25483ddcff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index, format='%d/%m/%Y %H:%M')\n",
    "data1 = df.loc['01/01/2021 12:00': '31/12/2023 11:59'].copy()\n",
    "data1.index = pd.to_datetime(data1.index)\n",
    "data1 = data1.resample(\"H\").sum()\n",
    "data1 = data1[[\"electricity_cons\", 'hour_sin','hour_cos','week_sin','week_cos','month_sin','month_cos','day_of_the_week_sin','day_of_the_week_cos','is_weekend']]\n",
    "\n",
    "# Choose colomns\n",
    "df = data1[[\"electricity_cons\", 'hour_sin','hour_cos','week_sin','week_cos','month_sin','month_cos','day_of_the_week_sin','day_of_the_week_cos','is_weekend']]\n",
    "\n",
    "# load and preprocess the energy dataset: \n",
    "def convert_col_into_float(df, list_cols):\n",
    "    for col in list_cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = df[col].str.replace(',', '.')\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    return df\n",
    "list_cols = list(df.columns)\n",
    "# gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data. (8 last features)\n",
    "print(\"dataset variables\", list_cols)\n",
    "df = convert_col_into_float(df, list_cols)\n",
    "data = df.values\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79324798-621e-4b18-967f-cbae2de05124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_into_seq(dataset, start_index=0, end_index=None, history_size=756, step=1, jump=1):\n",
    "    '''split the dataset to have sequence of observations of length history size'''\n",
    "    data = []\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset)\n",
    "    for i in range(start_index, end_index, jump):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d936f89-ff29-465b-912b-42b4a85573d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(data, TRAIN_SPLIT=0.7, VAL_SPLIT=0.1, save_path=None):\n",
    "    '''split the dataset into train, val and test splits'''\n",
    "    #data_in_seq = split_dataset_into_seq(data, start_index=0, end_index=None, history_size=60, step=1)\n",
    "    scaler_0 = StandardScaler()\n",
    "\n",
    "\n",
    "    # split between validation dataset and test set:\n",
    "    train_data = data[:17520-1752,:]\n",
    "    train_data[:,0] = scaler_0.fit_transform(train_data[:,0].reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "    train_data = split_dataset_into_seq(train_data, start_index=0, end_index=None, history_size=756, step=1, jump=1)\n",
    "    \n",
    "    val_data = data[17520-1752:17520,:]\n",
    "    val_data[:,0] = scaler_0.transform(val_data[:,0].reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "\n",
    "    val_data = split_dataset_into_seq(val_data, start_index=0, end_index=None, history_size=756, step=1, jump=24)\n",
    "    \n",
    "    test_data = data[17520:,:]\n",
    "    test_data[:,0] = scaler_0.transform(test_data[:,0].reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "    test_data = split_dataset_into_seq(test_data, start_index=0, end_index=None, history_size=756, step=1, jump=24)\n",
    "\n",
    "    \n",
    "    #train_data, val_data = train_test_split(data_in_seq, train_size=TRAIN_SPLIT, shuffle=False, random_state=123)\n",
    "    #val_data, test_data = train_test_split(val_data, train_size=VAL_SPLIT, shuffle=False, random_state=123)\n",
    "    #print(test_data.shape)\n",
    "    print(train_data.shape)\n",
    "    indices = torch.randperm(15012)\n",
    "    train_data = train_data[indices]\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_data, val_data, test_data, scaler_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf2b06-70e6-425e-98ce-3107c553946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fn(chunk):\n",
    "    \"\"\"to split the dataset sequences into input and targets sequences\"\"\"\n",
    "    inputs = torch.tensor(chunk[:, :720, :], device=device)\n",
    "    targets = torch.tensor(chunk[:, 36:, :], device=device)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66dff4-b347-41a4-94b2-fa5c53bfc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dataset(train_data, val_data, test_data, batch_size=32, target_features=0):\n",
    "    '''\n",
    "    split each train split into inputs and targets \n",
    "    convert each train split into a tf.dataset\n",
    "    '''\n",
    "    x_train, y_train = split_fn(train_data)\n",
    "    x_val, y_val = split_fn(val_data)\n",
    "    x_test, y_test = split_fn(test_data)\n",
    "    \n",
    "    # selecting only the first 1 features for prediction: \n",
    "    y_train = y_train[:, :, target_features]\n",
    "    y_val = y_val[:, :, target_features]\n",
    "    y_test = y_test[:, :, target_features]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dae04-8aa6-435c-bd26-b91dff74c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "train_data, val_data, test_data, scaler = split_dataset(data)\n",
    "\n",
    "\n",
    "print(range(len(test_data[:,1,1])))\n",
    "train_dataset, val_dataset, test_dataset = data_to_dataset(train_data, val_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e08bb6-fa9f-429e-bfda-d4112ee88014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multi-head self-attention module'''\n",
    "    def __init__(self, D, H):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.H = H # number of heads\n",
    "        self.D = D # dimension\n",
    "        \n",
    "        self.wq = nn.Linear(D, D*H)\n",
    "        self.wk = nn.Linear(D, D*H)\n",
    "        self.wv = nn.Linear(D, D*H)\n",
    "\n",
    "        self.dense = nn.Linear(D*H, D)\n",
    "\n",
    "    def concat_heads(self, x):\n",
    "        '''(B, H, S, D) => (B, S, D*H)'''\n",
    "        B, H, S, D = x.shape\n",
    "        x = x.permute((0, 2, 1, 3)).contiguous()  # (B, S, H, D)\n",
    "        x = x.reshape((B, S, H*D))   # (B, S, D*H)\n",
    "        return x\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        '''(B, S, D*H) => (B, H, S, D)'''\n",
    "        B, S, D_H = x.shape\n",
    "        x = x.reshape(B, S, self.H, self.D)    # (B, S, H, D)\n",
    "        x = x.permute((0, 2, 1, 3))  # (B, H, S, D)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        q = self.wq(x)  # (B, S, D*H)\n",
    "        k = self.wk(x)  # (B, S, D*H)\n",
    "        v = self.wv(x)  # (B, S, D*H)\n",
    "\n",
    "        q = self.split_heads(q)  # (B, H, S, D)\n",
    "        k = self.split_heads(k)  # (B, H, S, D)\n",
    "        v = self.split_heads(v)  # (B, H, S, D)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2)) #(B,H,S,S)\n",
    "        attention_scores = attention_scores / math.sqrt(self.D)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            attention_scores += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n",
    "        scaled_attention = torch.matmul(attention_weights, v)  # (B, H, S, D)\n",
    "        concat_attention = self.concat_heads(scaled_attention) # (B, S, D*H)\n",
    "        output = self.dense(concat_attention)  # (B, S, D)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa58e0-1b2b-4d11-b7c0-e3cf8d46bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encodings\n",
    "def get_angles(pos, i, D):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(D, position=720, dim=3, device=device):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(D)[np.newaxis, :],\n",
    "                            D)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    if dim == 3:\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    elif dim == 4:\n",
    "        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n",
    "    return torch.tensor(pos_encoding, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef195a-89ff-4d13-a1a2-a0f8e0a1fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that implement the look_ahead mask for masking future time steps. \n",
    "def create_look_ahead_mask(size, device=device):\n",
    "    mask = torch.ones((size, size), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    return mask  # (size, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07b041-b13d-4a9e-91af-5c621152d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, H, D = 9, 11, 5, 8\n",
    "mha = MultiHeadAttention(D, H)\n",
    "out, att = mha.forward(torch.zeros(B, S, D), mask=None)\n",
    "out.shape, att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37c2e0-4a2d-4560-b342-739932b1df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n",
    "        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n",
    "        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n",
    "        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.mha = MultiHeadAttention(D, H)\n",
    "\n",
    "\n",
    "    def forward(self, x, look_ahead_mask):\n",
    "        \n",
    "        attn, attn_weights = self.mha(x, look_ahead_mask)  # (B, S, D)\n",
    "        attn = self.dropout1(attn) # (B,S,D)\n",
    "        attn = self.layernorm1(attn + x) # (B,S,D)\n",
    "\n",
    "        mlp_act = torch.relu(self.mlp_hidden(attn))\n",
    "        mlp_act = self.mlp_out(mlp_act)\n",
    "        mlp_act = self.dropout2(mlp_act)\n",
    "        \n",
    "        output = self.layernorm2(mlp_act + attn)  # (B, S, D)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba977ad-a808-4523-bc0b-b2a605fe0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = TransformerLayer(16, 3, 32, 0.1)\n",
    "out, attn = dl(x=torch.zeros(5, 7, 16), look_ahead_mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91878206-32df-404b-903e-3f726cbb1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''Transformer Decoder Implementating several Decoder Layers.\n",
    "    '''\n",
    "    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features, out_features, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.sqrt_D = torch.tensor(math.sqrt(D))\n",
    "        self.num_layers = num_layers\n",
    "        self.input_projection = nn.Linear(inp_features, D) # multivariate input\n",
    "        self.output_projection = nn.Linear(D, out_features) # multivariate output\n",
    "        self.pos_encoding = positional_encoding(D)\n",
    "        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim, \n",
    "                                        dropout_rate=dropout_rate\n",
    "                                       ) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, S, D = x.shape\n",
    "        attention_weights = {}\n",
    "        x = self.input_projection(x)\n",
    "        x *= self.sqrt_D\n",
    "        \n",
    "        x += self.pos_encoding[:, :S, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block = self.dec_layers[i](x=x,\n",
    "                                          look_ahead_mask=mask)\n",
    "            attention_weights['decoder_layer{}'.format(i + 1)] = block\n",
    "        \n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        return x, attention_weights # (B,S,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a61967-fe8d-41e2-87aa-954ee9083b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Forward pass on the Transformer: \n",
    "transformer = Transformer(num_layers=1, D=32, H=1, hidden_mlp_dim=32,\n",
    "                                       inp_features=10, out_features=1, dropout_rate=0.1)\n",
    "transformer.to(device)\n",
    "(inputs, targets) = next(iter(train_dataset))\n",
    "                         \n",
    "S = inputs.shape[1]\n",
    "print(S)\n",
    "mask = create_look_ahead_mask(S)\n",
    "out, attn = transformer (x=inputs, mask=mask)\n",
    "out.shape, attn[\"decoder_layer1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db458d-a513-401a-8e42-7f92d0eef699",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sizes = [p.numel() for p in transformer.parameters()]\n",
    "print(f\"number of weight/biases matrices: {len(param_sizes)} \"\n",
    "      f\"for a total of {np.sum(param_sizes)} parameters \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e1116-1818-498c-9054-8fab4f6c2f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers=3, D=32, H=8, hidden_mlp_dim=32,\n",
    "                          inp_features=10, out_features=1, dropout_rate=0.1).to(device)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), \n",
    "                                lr=0.000001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a5fcf-f789-4efb-bd20-0c388ba45ad6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 30\n",
    "niter = len(train_dataset)\n",
    "losses, val_losses = [], []\n",
    "early_stop_count = 0\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for e in tqdm(range(n_epochs)):\n",
    "    \n",
    "    # one epoch on train set\n",
    "    transformer.train()\n",
    "    sum_train_loss = 0.0\n",
    "    for x,y in train_dataset:\n",
    "        y = y.unsqueeze(-1) # tilføjet\n",
    "        S = x.shape[1]\n",
    "        mask = create_look_ahead_mask(S)\n",
    "        out, _ = transformer(x, mask)\n",
    "        loss = torch.nn.MSELoss()(out[-24:], y[-24:])\n",
    "        sum_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(sum_train_loss / niter)\n",
    "    \n",
    "    # Evaluate on val set\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        sum_val_loss = 0.0\n",
    "        for i, (x, y) in enumerate(val_dataset):\n",
    "            y = y.unsqueeze(-1) # tilføjet\n",
    "            S = x.shape[1]\n",
    "            mask = create_look_ahead_mask(S)\n",
    "            out, _ = transformer(x, mask)\n",
    "            loss = torch.nn.MSELoss()(out[-24:], y[-24:])\n",
    "            sum_val_loss += loss.item()\n",
    "        val_losses.append(sum_val_loss / (i + 1))\n",
    "\n",
    "\n",
    "    if val_losses[e] < min_val_loss:\n",
    "        min_val_loss = val_losses[e]\n",
    "        early_stop_count = 0\n",
    "        torch.save(transformer.state_dict(), 'Models/transformer_{}.pth'.format(name))\n",
    "    elif early_stop_count >= 5:\n",
    "        print(\"Early stopping!\")  \n",
    "        break\n",
    "        print(e)\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc71196-272b-4938-af7b-46ebd5390e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(losses, label='Training Loss',color=\"red\")\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# set the basic lables\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Model Loss')\n",
    "\n",
    "# tweak the title\n",
    "ttl = ax.title\n",
    "ttl.set_weight('bold')\n",
    "\n",
    "# tweak the axis labels\n",
    "xlab = ax.xaxis.get_label()\n",
    "ylab = ax.yaxis.get_label()\n",
    "xlab.set_style('italic')\n",
    "xlab.set_size(10)\n",
    "ylab.set_style('italic')\n",
    "ylab.set_size(10)\n",
    "\n",
    "# grid on\n",
    "ax.grid('on', linestyle = \"--\", alpha=0.5)\n",
    "\n",
    "# color of plot, just to be sure\n",
    "ax.set_facecolor('xkcd:white')\n",
    "\n",
    "# change the color of the top and right spines to opaque gray\n",
    "ax.spines['right'].set_color((.8,.8,.8))\n",
    "ax.spines['top'].set_color((.8,.8,.8))\n",
    "\n",
    "\n",
    "plt.savefig('epochs/{}.png'.format(name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f34b0-1035-4cee-bf99-9337a9268839",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "transformer = Transformer(num_layers=3, D=32, H=8, hidden_mlp_dim=32,\n",
    "                          inp_features=10, out_features=1, dropout_rate=0.1).to(device)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), \n",
    "                                lr=0.000001)\n",
    "\n",
    "transformer.load_state_dict(torch.load('Models/transformer_{}.pth'.format(name)))\n",
    "\n",
    "\n",
    "# Define function to retrain model on new data\n",
    "def retrain_model(transformer, optimizer, train_dataset, num_epochs):\n",
    "    transformer.train()  # Set model to training mode\n",
    "\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        # one epoch on train set\n",
    "        transformer.train()\n",
    "        sum_train_loss = 0.0\n",
    "        for x,y in train_dataset:\n",
    "            y = y.unsqueeze(-1) # tilføjet\n",
    "            S = x.shape[1]\n",
    "            mask = create_look_ahead_mask(S)\n",
    "            out, _ = transformer(x, mask)\n",
    "            loss = torch.nn.MSELoss()(out[-24:], y[-24:])\n",
    "            sum_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "     \n",
    "\n",
    "# Define rolling window parameters\n",
    "window_size = 7  # One week's worth of data\n",
    "stride = 7  # Advance one week at a time\n",
    "num_weeks = (365-(9+7+7+7)) // stride\n",
    "print(num_weeks)\n",
    "\n",
    "\n",
    "# Initialize lists to store predictions and losses\n",
    "test_losses = []\n",
    "test_preds = []\n",
    "\n",
    "# Evaluate model using rolling window approach\n",
    "for i in range(num_weeks):\n",
    "    \n",
    "    start_idx = i * stride\n",
    "    end_idx = start_idx + window_size\n",
    "\n",
    "       \n",
    "    test_subset = test_data[start_idx:end_idx,:,:]\n",
    "\n",
    "    if i == 0: \n",
    "        new_train, _, test_subset = data_to_dataset(train_data, val_data, test_subset)\n",
    "        \n",
    "    if i != 0:\n",
    "        new_train = np.append(test_subset,train_data,axis=0)\n",
    "    \n",
    "        new_train, _, test_subset = data_to_dataset(new_train, val_data, test_subset)\n",
    "\n",
    "    \n",
    "    # Evaluate model on current window\n",
    "    transformer.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        window_losses = []\n",
    "        window_preds = []\n",
    "        for (x, y) in test_subset:\n",
    "            y = y.unsqueeze(-1)\n",
    "            S = x.shape[-2]\n",
    "            y_pred, attention = transformer(x, mask=create_look_ahead_mask(S))\n",
    "            loss = torch.nn.MSELoss()(y_pred, y)\n",
    "            window_losses.append(loss.item())\n",
    "            window_preds.append(y_pred.detach().cpu().numpy())\n",
    "        test_losses.extend(window_losses)\n",
    "        test_preds.extend(window_preds)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Retrain model on new data for this window\n",
    "    transformer.train()  # Ensure model is in training mode\n",
    "    retrain_model(transformer, optimizer, new_train, num_epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a816a0d-98a6-4278-ad40-2ef026516fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = test_preds\n",
    "test_preds = np.vstack(save).copy()\n",
    "print(test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ddf7d-2b2b-4820-9045-71772225fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more daily forecasts against expected values\n",
    "def evaluate_forecast(actual, predicted):\n",
    "    scores = list()\n",
    "    \n",
    "\t# calculate an RMSE score for each hour\n",
    "    for i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "\t\t# store (changed to mse since it otherwise would not make sense on the plot)\n",
    "        scores.append(rmse)\n",
    "    \n",
    "\t# calculate overall RMSE\n",
    "    sum = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            sum += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(sum / (actual.shape[0] * actual.shape[1]))\n",
    "    \n",
    "    # calculate error distribution\n",
    "    error_dist = np.zeros_like(actual, dtype=float)\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            squared_error = (actual[row, col] - predicted[row, col])**2\n",
    "            error_dist[row, col] = sqrt(squared_error)\n",
    "    return score, scores, error_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764910b4-0b67-4f46-8a2c-caad0315b813",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "def performance(name, score, scores):\n",
    " use_scores = ', '.join(['%.1f' % s for s in scores])\n",
    " print('%s: [%.3f] %s' % (name, score, use_scores))\n",
    "    \n",
    " # Creating a dictionary of the data\n",
    " data = {'Model': [name],\n",
    "        'Overall RMSE-score': [score]}\n",
    "\n",
    " # Dynamically create columns for scores\n",
    " data.update({f'RMSE Score {i}': [scores[i-1]] for i in range(1, 25)})\n",
    "\n",
    " # Creating a DataFrame from the dictionary\n",
    " df = pd.DataFrame(data)\n",
    "    \n",
    " # Appending the new data to the DataFrame\n",
    " df.to_csv('Transformer_{}_output.csv'.format(data_type), mode='a', index=False, header=False)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b190d42-8f8d-4b19-86a6-002c7e636bbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For plotting\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import datetime\n",
    "from math import sqrt\n",
    "\n",
    "pio.renderers.default='browser'\n",
    "pio.templates.default = \"seaborn\"\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "\n",
    "\n",
    "predictions = np.array(test_preds) \n",
    "\n",
    "actuals = test_data[0:329,:,:]\n",
    "\n",
    "\n",
    "_, _, actuals = data_to_dataset(train_data, val_data, actuals)\n",
    "        \n",
    "\n",
    "_, actuals = actuals.dataset.tensors\n",
    "actualss = np.array(actuals) \n",
    "\n",
    "\n",
    "actualss = actualss[:,-24:] \n",
    "predictions = predictions[:,-24:,:] \n",
    "\n",
    "actualss = actualss.reshape(-1, 1)\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "\n",
    "actualss = scaler.inverse_transform(actualss)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the array into 24-hour sequences\n",
    "num_hours_per_day = 24\n",
    "num_days = actualss.size // num_hours_per_day\n",
    "actualss = actualss.reshape(num_days, num_hours_per_day)\n",
    "predictions = predictions.reshape(num_days, num_hours_per_day)\n",
    "\n",
    "\n",
    "score, scores, error_dist = evaluate_forecast(actualss, predictions)\n",
    "\n",
    "\n",
    "performance(name, score, scores)\n",
    "\n",
    "\n",
    "predictions = predictions.flatten()\n",
    "#predictions = predictions[:-12]\n",
    "\n",
    "actuals = actualss.flatten()\n",
    "#actuals = actuals[12:]\n",
    "\n",
    "result = pd.DataFrame({'preds': predictions, 'actuals': actuals})\n",
    "\n",
    "start_date = '2023-02-05 00:00:00'\n",
    "end_date = '2023-12-30 23:00:00'\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "\n",
    "# Create datetime index\n",
    "datetime_index = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "# Create DataFrame with datetime index, predictions, and actuals\n",
    "resultss = pd.DataFrame({'Datetime': datetime_index, 'Preds': predictions, 'Actuals': actuals})\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "resultss.to_csv('Predictions/{}_predictions_actuals.csv'.format(name), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_datetime = datetime.datetime.strptime(start_date, date_format)\n",
    "end_datetime = datetime.datetime.strptime(end_date, date_format)\n",
    "\n",
    "date_range = pd.date_range(start=start_datetime, end=end_datetime, freq='H')\n",
    "\n",
    "\n",
    "\n",
    "result = result.set_index(date_range)\n",
    "\n",
    "fig = go.Figure([\n",
    "    go.Scatter(\n",
    "        name='Prediction',\n",
    "        x=result.index,\n",
    "        y=result[\"preds\"],\n",
    "        mode='lines',\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        name='Real value',\n",
    "        x=result.index,\n",
    "        y=result[\"actuals\"],\n",
    "        mode='lines',\n",
    "     )\n",
    "])\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Real value vs predicted in test data\",\n",
    "    xaxis_title=\"Date time\",\n",
    "    yaxis_title=\"Demand\",\n",
    "    plot_bgcolor='white',\n",
    "    width=800,\n",
    "    height=400,\n",
    "    margin=dict(l=20, r=20, t=35, b=20),\n",
    "    hovermode=\"x\",\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"top\",\n",
    "        y=1.1,\n",
    "        xanchor=\"left\",\n",
    "        x=0.001\n",
    "    ),\n",
    "    xaxis=dict(linecolor='black',showgrid=True, gridcolor='rgba(0, 0, 0, 0.3)',griddash='dash',mirror=True),\n",
    "    yaxis=dict(linecolor='black',showgrid=True, gridcolor='rgba(0, 0, 0, 0.3)',griddash='dash',mirror=True),\n",
    ")\n",
    "\n",
    "\n",
    "fig.write_html('Predictions/Transformer_{}.html'.format(name), auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071cc37c-9e68-43c2-beae-cc012e9ec151",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot scores\n",
    "\n",
    "hours = ['00', '01', '02', '03', '04', '05', '06','07', '08', '09', '10', '11', '12', '13','14', '15', '16', '17', '18', '19', '20','21', '22', '23']\n",
    "plt.figure(figsize=(9, 5))\n",
    "\n",
    "boxplot_positions = np.arange(len(hours))\n",
    "# Add boxplots for each column in the error_dist array\n",
    "for col in range(error_dist.shape[1]):\n",
    "    boxplot_values = error_dist[:, col]\n",
    "    plt.boxplot(boxplot_values, positions=[col], widths=0.4, showfliers=False, patch_artist=True, medianprops=dict(color='black'))\n",
    "\n",
    "\n",
    "plt.plot(hours, scores, marker='o', label='lstm', color=\"red\")\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# set the basic lables\n",
    "ax.set_xlabel('Hours')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Average hourly RMSE')\n",
    "\n",
    "# tweak the title\n",
    "ttl = ax.title\n",
    "ttl.set_weight('bold')\n",
    "\n",
    "# tweak the axis labels\n",
    "xlab = ax.xaxis.get_label()\n",
    "ylab = ax.yaxis.get_label()\n",
    "xlab.set_style('italic')\n",
    "xlab.set_size(10)\n",
    "ylab.set_style('italic')\n",
    "ylab.set_size(10)\n",
    "\n",
    "# grid on\n",
    "ax.grid('on', linestyle = \"--\", alpha=0.5)\n",
    "\n",
    "# color of plot, just to be sure\n",
    "ax.set_facecolor('xkcd:white')\n",
    "\n",
    "# change the color of the top and right spines to opaque gray\n",
    "ax.spines['right'].set_color((.8,.8,.8))\n",
    "ax.spines['top'].set_color((.8,.8,.8))\n",
    "\n",
    "\n",
    "plt.savefig('scores/scores_{}.png'.format(name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39392f-1ef5-4bd6-99c2-02794d53d5ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
